{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model\n",
    "- Miller et al. 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "\n",
    "npast = 10\n",
    "params_pooled = []\n",
    "task_type_bool = []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type = \"unstructured\" if prob_corr < 0.2 else \"structured\"\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy()\n",
    "    choices[choices == 2] = -1\n",
    "    outcomes = data_df[\"reward\"].to_numpy()\n",
    "    outcomes[outcomes == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    past_choices = sliding_window_view(choices, npast)[:-1, :]\n",
    "    past_outcomes = sliding_window_view(outcomes, npast)[:-1, :]\n",
    "    actual_choices = choices[npast:]\n",
    "\n",
    "    x = np.hstack(\n",
    "        (\n",
    "            past_choices * past_outcomes,\n",
    "            past_choices,\n",
    "            past_outcomes,\n",
    "        )\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0).fit(x, actual_choices)\n",
    "\n",
    "    params = np.fliplr(clf.coef_.squeeze().reshape(3, npast))\n",
    "    params_pooled.append(params)\n",
    "\n",
    "    subfig = fig.add_subfigure(fig.gs[i])\n",
    "    subfig.suptitle(f\"{files[i].name[:-4]}, {task_type}\")\n",
    "    sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "    colors = [\"orange\", \"purple\", \"blue\"]\n",
    "    titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "    for _, ax in enumerate(sub_axs):\n",
    "\n",
    "        ax.plot(np.arange(1, 11), params[_], \".-\", color=colors[_], zorder=1)\n",
    "        ax.set_title(titles[_])\n",
    "        ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "        ax.set_xticks([1, 5, 10])\n",
    "\n",
    "    if i == 0:\n",
    "        sub_axs[0].set_xlabel(\"Trials in the past\")\n",
    "        sub_axs[0].set_ylabel(\"Influence on current choice\")\n",
    "\n",
    "task_type_bool = np.array(task_type_bool)\n",
    "params_pooled = np.array(params_pooled)\n",
    "mean_struc = params_pooled[task_type_bool < 0.2, :, :].mean(axis=0)\n",
    "mean_unstruc = params_pooled[task_type_bool > 0.2, :, :].mean(axis=0)\n",
    "\n",
    "subfig = fig.add_subfigure(fig.gs[4:, 0:2])\n",
    "subfig.suptitle(f\"Mean across animals by task type\")\n",
    "sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "# colors = [\"orange\", \"purple\", \"blue\"]\n",
    "colors = [\"#5040BF\", \"#AFBF40\"]\n",
    "\n",
    "\n",
    "titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "for _, ax in enumerate(sub_axs):\n",
    "\n",
    "    ax.plot(np.arange(1, 11), mean_struc[_], \".-\", color=colors[0], alpha=0.7, zorder=1)\n",
    "    ax.plot(\n",
    "        np.arange(1, 11), mean_unstruc[_], \".-\", color=colors[1], alpha=0.7, zorder=1\n",
    "    )\n",
    "    ax.legend([\"Struc\", \"Unstruc\"])\n",
    "    ax.set_title(titles[_])\n",
    "    ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "    ax.set_xticks([1, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cognitive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "\n",
    "npast = 10\n",
    "params_pooled = []\n",
    "task_type_bool = []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type = \"unstructured\" if prob_corr < 0.2 else \"structured\"\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy()\n",
    "    choices[choices == 2] = -1\n",
    "    outcomes = data_df[\"reward\"].to_numpy()\n",
    "    outcomes[outcomes == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    past_choices = sliding_window_view(choices, npast)[:-1, :]\n",
    "    past_outcomes = sliding_window_view(outcomes, npast)[:-1, :]\n",
    "    actual_choices = choices[npast:]\n",
    "\n",
    "    x = np.hstack(\n",
    "        (\n",
    "            past_choices * past_outcomes,\n",
    "            past_choices,\n",
    "            past_outcomes,\n",
    "        )\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0).fit(x, actual_choices)\n",
    "\n",
    "    params = np.fliplr(clf.coef_.squeeze().reshape(3, npast))\n",
    "    params_pooled.append(params)\n",
    "\n",
    "    subfig = fig.add_subfigure(fig.gs[i])\n",
    "    subfig.suptitle(f\"{files[i].name[:-4]}, {task_type}\")\n",
    "    sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "    colors = [\"orange\", \"purple\", \"blue\"]\n",
    "    titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "    for _, ax in enumerate(sub_axs):\n",
    "\n",
    "        ax.plot(np.arange(1, 11), params[_], \".-\", color=colors[_], zorder=1)\n",
    "        ax.set_title(titles[_])\n",
    "        ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "        ax.set_xticks([1, 5, 10])\n",
    "\n",
    "    if i == 0:\n",
    "        sub_axs[0].set_xlabel(\"Trials in the past\")\n",
    "        sub_axs[0].set_ylabel(\"Influence on current choice\")\n",
    "\n",
    "task_type_bool = np.array(task_type_bool)\n",
    "params_pooled = np.array(params_pooled)\n",
    "mean_struc = params_pooled[task_type_bool < 0.2, :, :].mean(axis=0)\n",
    "mean_unstruc = params_pooled[task_type_bool > 0.2, :, :].mean(axis=0)\n",
    "\n",
    "subfig = fig.add_subfigure(fig.gs[4:, 0:2])\n",
    "subfig.suptitle(f\"Mean across animals by task type\")\n",
    "sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "colors = [\"orange\", \"purple\", \"blue\"]\n",
    "titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "for _, ax in enumerate(sub_axs):\n",
    "\n",
    "    ax.plot(np.arange(1, 11), mean_struc[_], \".-\", color=colors[_], zorder=1)\n",
    "    ax.plot(\n",
    "        np.arange(1, 11), mean_unstruc[_], \".-\", color=colors[_], alpha=0.5, zorder=1\n",
    "    )\n",
    "    ax.set_title(titles[_])\n",
    "    ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "    ax.set_xticks([1, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asheshlab\\Documents\\Codes\\NeuroPy\\neuropy\\plotting\\figure.py:162: UserWarning: Ignoring specified arguments in this call because figure with num: 1 already exists\n",
      "  fig = plt.figure(num=num, figsize=(8.5, 11), clear=True)\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_14388\\970062793.py:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha: 0.1180\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Simulated Example Data (Each row: [choice, reward])\n",
    "# data = np.array([[0, 1], [1, 0], [1, 1], [0, 0], [0, 1], [1, 0], [1, 1], [0, 0]])\n",
    "\n",
    "# choices = data[:, 0]  # 0 or 1 (action taken)\n",
    "# rewards = data[:, 1]  # 0 or 1 (reward received)\n",
    "\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "\n",
    "npast = 10\n",
    "params_pooled = []\n",
    "task_type_bool = []\n",
    "\n",
    "for i, file in enumerate(files[:1]):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type = \"unstructured\" if prob_corr < 0.2 else \"structured\"\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy().astype(int)\n",
    "    choices[choices == 2] = 0\n",
    "    rewards = data_df[\"reward\"].to_numpy().astype(int)\n",
    "    # rewards[rewards == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    # Q-learning function with given alpha\n",
    "    def compute_q_values(alpha):\n",
    "        Q = np.zeros(2)  # Initialize Q-values for two actions\n",
    "        q_values = []\n",
    "\n",
    "        for choice, reward in zip(choices, rewards):\n",
    "            Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
    "            q_values.append(Q.copy())\n",
    "\n",
    "        return np.array(q_values)\n",
    "\n",
    "    # Loss function to optimize alpha (maximize log-likelihood)\n",
    "    def log_likelihood(alpha):\n",
    "        Q_values = compute_q_values(alpha)\n",
    "        X = (Q_values[:, 0] - Q_values[:, 1]).reshape(-1, 1)  # Difference in Q-values\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, choices)  # Fit logistic regression on choice data\n",
    "\n",
    "        # Compute log-likelihood\n",
    "        probs = model.predict_proba(X)[:, 1]  # Probability of choosing action 1\n",
    "        ll = np.sum(choices * np.log(probs) + (1 - choices) * np.log(1 - probs))\n",
    "        return -ll  # Negative for minimization\n",
    "\n",
    "    # Optimize alpha using a bounded method\n",
    "    result = minimize(log_likelihood, x0=0.5, bounds=[(0, 1)], method=\"L-BFGS-B\")\n",
    "\n",
    "    alpha_estimated = result.x[0]\n",
    "    print(f\"Estimated alpha: {alpha_estimated:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated alpha_L: 0.0921, Estimated alpha_R: 0.1707, Estimated: beta: 5.483222442460307\n",
      "Estimated alpha_L: 0.1736, Estimated alpha_R: 0.9315, Estimated: beta: 2.796233073343137\n",
      "Estimated alpha_L: 0.1640, Estimated alpha_R: 0.3838, Estimated: beta: 3.178154749857273\n",
      "Estimated alpha_L: 0.0547, Estimated alpha_R: 0.0645, Estimated: beta: 8.093633697850429\n",
      "Estimated alpha_L: 0.2819, Estimated alpha_R: 0.0004, Estimated: beta: 10.0\n",
      "Estimated alpha_L: 0.1663, Estimated alpha_R: 0.1569, Estimated: beta: 7.502534759056712\n",
      "Estimated alpha_L: 0.1316, Estimated alpha_R: 0.0834, Estimated: beta: 5.057157500537784\n",
      "Estimated alpha_L: 0.0745, Estimated alpha_R: 0.1509, Estimated: beta: 7.607211627021995\n",
      "Estimated alpha_L: 0.2599, Estimated alpha_R: 0.0607, Estimated: beta: 4.530078434819735\n",
      "Estimated alpha_L: 0.2299, Estimated alpha_R: 0.1507, Estimated: beta: 4.972490685682155\n",
      "Estimated alpha_L: 0.1678, Estimated alpha_R: 0.1235, Estimated: beta: 4.498972310981454\n",
      "Estimated alpha_L: 0.2979, Estimated alpha_R: 0.3262, Estimated: beta: 1.977029146138243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "# fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "estimated_params = []\n",
    "task_type = []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type.append(\"unstructured\" if prob_corr < 0.2 else \"structured\")\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy().astype(int)\n",
    "    choices[choices == 2] = 0\n",
    "    rewards = data_df[\"reward\"].to_numpy().astype(int)\n",
    "    # rewards[rewards == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    # Q-learning function with different learning rates for left and right\n",
    "    def compute_q_values(alpha_L, alpha_R):\n",
    "        Q = np.zeros(2)  # Q-values: Q[0] for Left, Q[1] for Right\n",
    "        q_values = []\n",
    "\n",
    "        for choice, reward in zip(choices, rewards):\n",
    "            if choice == 0:\n",
    "                Q[0] += alpha_L * (reward - Q[0])\n",
    "            else:\n",
    "                Q[1] += alpha_R * (reward - Q[1])\n",
    "            q_values.append(Q.copy())\n",
    "\n",
    "        return np.array(q_values)\n",
    "\n",
    "    # Log-likelihood function to optimize alpha_L and alpha_R\n",
    "    def log_likelihood(params):\n",
    "        alpha_L, alpha_R, beta = params\n",
    "        Q_values = compute_q_values(alpha_L, alpha_R)\n",
    "\n",
    "        # Predictor for logistic regression: difference in Q-values (Q_right - Q_left)\n",
    "        # X = (Q_values[:, 1] - Q_values[:, 0]).reshape(-1, 1)\n",
    "        # model = LogisticRegression()\n",
    "        # model.fit(X, choices)  # Fit logistic regression on choice data\n",
    "        # probs = model.predict_proba(X)[:, 1]  # Probability of choosing right (1)\n",
    "\n",
    "        Q_diff = Q_values[:, 1] - Q_values[:, 0]\n",
    "        probs = 1 / (1 + np.exp(-beta * Q_diff))  # Softmax choice probability\n",
    "\n",
    "        # Compute log-likelihood\n",
    "        ll = np.sum(choices * np.log(probs) + (1 - choices) * np.log(1 - probs))\n",
    "        return -ll  # Negative for minimization\n",
    "\n",
    "    # Optimize alpha_L and alpha_R using a bounded method\n",
    "    result = minimize(\n",
    "        log_likelihood,\n",
    "        x0=[0.5, 0.5, 1],\n",
    "        bounds=[(0, 1), (0, 1), (0, 10)],\n",
    "        method=\"L-BFGS-B\",\n",
    "    )\n",
    "\n",
    "    estimated_params.append(result.x)\n",
    "    alpha_L_est, alpha_R_est, beta = result.x\n",
    "    print(\n",
    "        f\"Estimated alpha_L: {alpha_L_est:.4f}, Estimated alpha_R: {alpha_R_est:.4f}, Estimated: beta: {beta}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asheshlab\\Documents\\Codes\\NeuroPy\\neuropy\\plotting\\figure.py:162: UserWarning: Ignoring specified arguments in this call because figure with num: 1 already exists\n",
      "  fig = plt.figure(num=num, figsize=(8.5, 11), clear=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Q-learning in two-armed bandit task')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plotting.Fig(1, 2, size=(4, 3), num=1)\n",
    "estimated_params = np.array(estimated_params)\n",
    "ax1 = fig.subplot(fig.gs[0])\n",
    "ax2 = fig.subplot(fig.gs[1])\n",
    "\n",
    "for i in range(estimated_params.shape[0]):\n",
    "    if task_type[i] == \"structured\":\n",
    "        color = \"#5040BF\"\n",
    "    else:\n",
    "        color = \"#AFBF40\"\n",
    "\n",
    "    x1 = np.array([1, 2]) + 0.1 * np.random.randn(2)\n",
    "\n",
    "    ax1.plot(x1, estimated_params[i, :2], \".\", color=color, alpha=0.6)\n",
    "    ax1.set_xlim(0, 3)\n",
    "    ax2.plot(\n",
    "        1 + 0.1 * np.random.randn(1),\n",
    "        estimated_params[i, 2],\n",
    "        \".\",\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    ax2.set_xlim(0, 2)\n",
    "\n",
    "# ax1.legend([\"struc\", \"unstruc\"])\n",
    "ax1.set_xticks([1, 2], [\"Alpha_L\", \"Alpha_R\"])\n",
    "ax2.set_xticks([1], [\"Beta\"])\n",
    "ax1.set_ylabel(\"Estimated alpha values\")\n",
    "ax2.set_ylabel(\"Estimated beta values\")\n",
    "fig.fig.suptitle(\"Q-learning in two-armed bandit task\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
