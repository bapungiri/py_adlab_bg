{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model\n",
    "- Miller et al. 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "\n",
    "npast = 10\n",
    "params_pooled = []\n",
    "task_type_bool = []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type = \"unstructured\" if prob_corr < 0.2 else \"structured\"\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy()\n",
    "    choices[choices == 2] = -1\n",
    "    outcomes = data_df[\"reward\"].to_numpy()\n",
    "    outcomes[outcomes == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    past_choices = sliding_window_view(choices, npast)[:-1, :]\n",
    "    past_outcomes = sliding_window_view(outcomes, npast)[:-1, :]\n",
    "    actual_choices = choices[npast:]\n",
    "\n",
    "    x = np.hstack(\n",
    "        (\n",
    "            past_choices * past_outcomes,\n",
    "            past_choices,\n",
    "            past_outcomes,\n",
    "        )\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0).fit(x, actual_choices)\n",
    "\n",
    "    params = np.fliplr(clf.coef_.squeeze().reshape(3, npast))\n",
    "    params_pooled.append(params)\n",
    "\n",
    "    subfig = fig.add_subfigure(fig.gs[i])\n",
    "    subfig.suptitle(f\"{files[i].name[:-4]}, {task_type}\")\n",
    "    sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "    colors = [\"orange\", \"purple\", \"blue\"]\n",
    "    titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "    for _, ax in enumerate(sub_axs):\n",
    "\n",
    "        ax.plot(np.arange(1, 11), params[_], \".-\", color=colors[_], zorder=1)\n",
    "        ax.set_title(titles[_])\n",
    "        ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "        ax.set_xticks([1, 5, 10])\n",
    "\n",
    "    if i == 0:\n",
    "        sub_axs[0].set_xlabel(\"Trials in the past\")\n",
    "        sub_axs[0].set_ylabel(\"Influence on current choice\")\n",
    "\n",
    "task_type_bool = np.array(task_type_bool)\n",
    "params_pooled = np.array(params_pooled)\n",
    "mean_struc = params_pooled[task_type_bool < 0.2, :, :].mean(axis=0)\n",
    "mean_unstruc = params_pooled[task_type_bool > 0.2, :, :].mean(axis=0)\n",
    "\n",
    "subfig = fig.add_subfigure(fig.gs[4:, 0:2])\n",
    "subfig.suptitle(f\"Mean across animals by task type\")\n",
    "sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "# colors = [\"orange\", \"purple\", \"blue\"]\n",
    "colors = [\"#5040BF\", \"#AFBF40\"]\n",
    "\n",
    "\n",
    "titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "for _, ax in enumerate(sub_axs):\n",
    "\n",
    "    ax.plot(np.arange(1, 11), mean_struc[_], \".-\", color=colors[0], alpha=0.7, zorder=1)\n",
    "    ax.plot(\n",
    "        np.arange(1, 11), mean_unstruc[_], \".-\", color=colors[1], alpha=0.7, zorder=1\n",
    "    )\n",
    "    ax.legend([\"Struc\", \"Unstruc\"])\n",
    "    ax.set_title(titles[_])\n",
    "    ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "    ax.set_xticks([1, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cognitive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "\n",
    "npast = 10\n",
    "params_pooled = []\n",
    "task_type_bool = []\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type = \"unstructured\" if prob_corr < 0.2 else \"structured\"\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy()\n",
    "    choices[choices == 2] = -1\n",
    "    outcomes = data_df[\"reward\"].to_numpy()\n",
    "    outcomes[outcomes == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    past_choices = sliding_window_view(choices, npast)[:-1, :]\n",
    "    past_outcomes = sliding_window_view(outcomes, npast)[:-1, :]\n",
    "    actual_choices = choices[npast:]\n",
    "\n",
    "    x = np.hstack(\n",
    "        (\n",
    "            past_choices * past_outcomes,\n",
    "            past_choices,\n",
    "            past_outcomes,\n",
    "        )\n",
    "    )\n",
    "    clf = LogisticRegression(random_state=0).fit(x, actual_choices)\n",
    "\n",
    "    params = np.fliplr(clf.coef_.squeeze().reshape(3, npast))\n",
    "    params_pooled.append(params)\n",
    "\n",
    "    subfig = fig.add_subfigure(fig.gs[i])\n",
    "    subfig.suptitle(f\"{files[i].name[:-4]}, {task_type}\")\n",
    "    sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "    colors = [\"orange\", \"purple\", \"blue\"]\n",
    "    titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "    for _, ax in enumerate(sub_axs):\n",
    "\n",
    "        ax.plot(np.arange(1, 11), params[_], \".-\", color=colors[_], zorder=1)\n",
    "        ax.set_title(titles[_])\n",
    "        ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "        ax.set_xticks([1, 5, 10])\n",
    "\n",
    "    if i == 0:\n",
    "        sub_axs[0].set_xlabel(\"Trials in the past\")\n",
    "        sub_axs[0].set_ylabel(\"Influence on current choice\")\n",
    "\n",
    "task_type_bool = np.array(task_type_bool)\n",
    "params_pooled = np.array(params_pooled)\n",
    "mean_struc = params_pooled[task_type_bool < 0.2, :, :].mean(axis=0)\n",
    "mean_unstruc = params_pooled[task_type_bool > 0.2, :, :].mean(axis=0)\n",
    "\n",
    "subfig = fig.add_subfigure(fig.gs[4:, 0:2])\n",
    "subfig.suptitle(f\"Mean across animals by task type\")\n",
    "sub_axs = subfig.subplots(1, 3, width_ratios=[1, 1, 1], sharey=True, sharex=True)\n",
    "\n",
    "colors = [\"orange\", \"purple\", \"blue\"]\n",
    "titles = [\"Reward Seeking\", \"Choice Preservation\", \"Main effect of Outcome\"]\n",
    "for _, ax in enumerate(sub_axs):\n",
    "\n",
    "    ax.plot(np.arange(1, 11), mean_struc[_], \".-\", color=colors[_], zorder=1)\n",
    "    ax.plot(\n",
    "        np.arange(1, 11), mean_unstruc[_], \".-\", color=colors[_], alpha=0.5, zorder=1\n",
    "    )\n",
    "    ax.set_title(titles[_])\n",
    "    ax.axhline(0, color=\"gray\", zorder=0, lw=0.8)\n",
    "    ax.set_xticks([1, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Simulated Example Data (Each row: [choice, reward])\n",
    "# data = np.array([[0, 1], [1, 0], [1, 1], [0, 0], [0, 1], [1, 0], [1, 1], [0, 0]])\n",
    "\n",
    "# choices = data[:, 0]  # 0 or 1 (action taken)\n",
    "# rewards = data[:, 1]  # 0 or 1 (reward received)\n",
    "\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "\n",
    "npast = 10\n",
    "params_pooled = []\n",
    "task_type_bool = []\n",
    "\n",
    "for i, file in enumerate(files[:1]):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type = \"unstructured\" if prob_corr < 0.2 else \"structured\"\n",
    "    task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy().astype(int)\n",
    "    choices[choices == 2] = 0\n",
    "    rewards = data_df[\"reward\"].to_numpy().astype(int)\n",
    "    # rewards[rewards == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    # Q-learning function with given alpha\n",
    "    def compute_q_values(alpha):\n",
    "        Q = np.zeros(2)  # Initialize Q-values for two actions\n",
    "        q_values = []\n",
    "\n",
    "        for choice, reward in zip(choices, rewards):\n",
    "            Q[choice] = Q[choice] + alpha * (reward - Q[choice])\n",
    "            q_values.append(Q.copy())\n",
    "\n",
    "        return np.array(q_values)\n",
    "\n",
    "    # Loss function to optimize alpha (maximize log-likelihood)\n",
    "    def log_likelihood(alpha):\n",
    "        Q_values = compute_q_values(alpha)\n",
    "        X = (Q_values[:, 0] - Q_values[:, 1]).reshape(-1, 1)  # Difference in Q-values\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X, choices)  # Fit logistic regression on choice data\n",
    "\n",
    "        # Compute log-likelihood\n",
    "        probs = model.predict_proba(X)[:, 1]  # Probability of choosing action 1\n",
    "        ll = np.sum(choices * np.log(probs) + (1 - choices) * np.log(1 - probs))\n",
    "        return -ll  # Negative for minimization\n",
    "\n",
    "    # Optimize alpha using a bounded method\n",
    "    result = minimize(log_likelihood, x0=0.5, bounds=[(0, 1)], method=\"L-BFGS-B\")\n",
    "\n",
    "    alpha_estimated = result.x[0]\n",
    "    print(f\"Estimated alpha: {alpha_estimated:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bads:TooCloseBounds: For each variable, hard and plausible bounds should not be too close. Moving plausible bounds.\n",
      "Beginning optimization of a DETERMINISTIC objective function\n",
      "\n",
      " Iteration    f-count         f(x)           MeshScale          Method             Actions\n",
      "     0           2         88365.2               1                                 Uncertainty test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_23836\\695222060.py:58: RuntimeWarning: overflow encountered in scalar add\n",
      "  Q[unchosen] += alpha_u * (reward - Q[choice])  # Unchosen action decay\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_23836\\695222060.py:57: RuntimeWarning: invalid value encountered in scalar add\n",
      "  Q[choice] += alpha_c * (reward - Q[choice])  # Chosen action update\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_23836\\695222060.py:82: RuntimeWarning: overflow encountered in multiply\n",
      "  probs = 1 / (1 + np.exp(-beta * Q_diff))  # Softmax choice probability\n",
      "C:\\Users\\asheshlab\\AppData\\Local\\Temp\\ipykernel_23836\\695222060.py:82: RuntimeWarning: overflow encountered in exp\n",
      "  probs = 1 / (1 + np.exp(-beta * Q_diff))  # Softmax choice probability\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "FunctionLogger:InvalidFuncValue:\n            The returned function value must be a finite real-valued scalar\n            (returned value nan)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Optimize alpha_L and alpha_R using a bounded method\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# result = minimize(\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m#     log_likelihood,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[38;5;66;03m#     # method=\"BFGS\",\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m    104\u001b[39m bads = BADS(\n\u001b[32m    105\u001b[39m     log_likelihood,\n\u001b[32m    106\u001b[39m     x0=np.array([\u001b[32m0.3\u001b[39m, -\u001b[32m0.1\u001b[39m, \u001b[32m1.2\u001b[39m]),\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m     plausible_upper_bounds=np.array([\u001b[32m0.6\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m10\u001b[39m]),\n\u001b[32m    111\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m result = \u001b[43mbads\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m estimated_params.append(result.x)\n\u001b[32m    115\u001b[39m alpha_L_est, alpha_R_est, beta = result.x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Codes\\pybads\\pybads\\bads\\bads.py:1170\u001b[39m, in \u001b[36mBADS.optimize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28mself\u001b[39m.restarts = \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mrestarts\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m# Initialize gp\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m gp, Ns_gp, sn2hpd, hyp_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_optimization_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m.search_es_hedge = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# init search hedge to None\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33moutput_fcn\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Codes\\pybads\\pybads\\bads\\bads.py:1052\u001b[39m, in \u001b[36mBADS._init_optimization_\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28mself\u001b[39m.optim_state[\u001b[33m\"\u001b[39m\u001b[33mrandom_seed\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._init_random_seed_()\n\u001b[32m   1051\u001b[39m \u001b[38;5;66;03m# Evaluate starting point and initial mesh,\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_mesh_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# Change options for uncertainty handling\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.optim_state[\u001b[33m\"\u001b[39m\u001b[33muncertainty_handling_level\u001b[39m\u001b[33m\"\u001b[39m] > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Codes\\pybads\\pybads\\bads\\bads.py:1013\u001b[39m, in \u001b[36mBADS._init_mesh_\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1002\u001b[39m u1 = contraints_check(\n\u001b[32m   1003\u001b[39m     u1,\n\u001b[32m   1004\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_state[\u001b[33m\"\u001b[39m\u001b[33mlb_search\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1009\u001b[39m     \u001b[38;5;28mself\u001b[39m.non_box_cons,\n\u001b[32m   1010\u001b[39m )\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m u_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(u1)):\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_logger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m idx_yval = np.argmin(\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mself\u001b[39m.function_logger.Y[: \u001b[38;5;28mself\u001b[39m.function_logger.Xn + \u001b[32m1\u001b[39m]\n\u001b[32m   1017\u001b[39m )\n\u001b[32m   1018\u001b[39m \u001b[38;5;28mself\u001b[39m.u = \u001b[38;5;28mself\u001b[39m.function_logger.X[idx_yval].copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Codes\\pybads\\pybads\\function_logger\\function_logger.py:169\u001b[39m, in \u001b[36mFunctionLogger.__call__\u001b[39m\u001b[34m(self, x, record_duplicate_data)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.any(\n\u001b[32m    162\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m np.isscalar(fval_orig)\n\u001b[32m    163\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(fval_orig)\n\u001b[32m    164\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isreal(fval_orig)\n\u001b[32m    165\u001b[39m ):\n\u001b[32m    166\u001b[39m     error_message = \u001b[33m\"\"\"\u001b[39m\u001b[33mFunctionLogger:InvalidFuncValue:\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[33m    The returned function value must be a finite real-valued scalar\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[33m    (returned value \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message.format(\u001b[38;5;28mstr\u001b[39m(fval_orig)))\n\u001b[32m    171\u001b[39m \u001b[38;5;66;03m# Check returned function SD\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.he_noise_flag \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    173\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(fsd) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isreal(fsd) \u001b[38;5;129;01mor\u001b[39;00m fsd <= \u001b[32m0.0\u001b[39m\n\u001b[32m    174\u001b[39m ):\n",
      "\u001b[31mValueError\u001b[39m: FunctionLogger:InvalidFuncValue:\n            The returned function value must be a finite real-valued scalar\n            (returned value nan)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.optimize import minimize\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pybads import BADS\n",
    "\n",
    "basepath = Path(\"D:\\\\Data\")\n",
    "# files = [\"gronckle.csv\", \"grump.csv\"]\n",
    "files = sorted(basepath.glob(\"*.csv\"))\n",
    "\n",
    "# fig = plotting.Fig(6, 3, size=(12, 5), num=1)\n",
    "estimated_params = []\n",
    "task_type = []\n",
    "\n",
    "for i, file in enumerate(files[:1]):\n",
    "    data_df = pd.read_csv(basepath / file)\n",
    "    prob_corr = np.abs(\n",
    "        stats.pearsonr(data_df[\"rewprobfull1\"], data_df[\"rewprobfull2\"])[0]\n",
    "    )\n",
    "\n",
    "    task_type.append(\"unstructured\" if prob_corr < 0.2 else \"structured\")\n",
    "    # task_type_bool.append(prob_corr)\n",
    "\n",
    "    choices = data_df[\"port\"].to_numpy().astype(int)\n",
    "    choices[choices == 2] = 0\n",
    "    rewards = data_df[\"reward\"].to_numpy().astype(int)\n",
    "    # rewards[rewards == 0] = -1\n",
    "    n_trials = choices.size\n",
    "\n",
    "    # Q-learning function with different learning rates for left and right\n",
    "    # def compute_q_values(alpha_L, alpha_R):\n",
    "    #     Q = np.zeros(2)  # Q-values: Q[0] for Left, Q[1] for Right\n",
    "    #     q_values = []\n",
    "\n",
    "    #     for choice, reward in zip(choices, rewards):\n",
    "    #         if choice == 0:\n",
    "    #             Q[0] += alpha_L * (reward - Q[0])\n",
    "    #         else:\n",
    "    #             Q[1] += alpha_R * (reward - Q[1])\n",
    "    #         q_values.append(Q.copy())\n",
    "\n",
    "    #     return np.array(q_values)\n",
    "\n",
    "    def compute_q_values(alpha_c, alpha_u):\n",
    "        Q = np.zeros(2)  # Q-values: Q[0] for Left, Q[1] for Right\n",
    "        q_values = []\n",
    "        q_diff = []\n",
    "\n",
    "        for choice, reward in zip(choices, rewards):\n",
    "            unchosen = (\n",
    "                1 - choice\n",
    "            )  # If Left (0) is chosen, Right (1) is unchosen, and vice versa\n",
    "\n",
    "            # Update Q-values for chosen and unchosen arms\n",
    "            Q[choice] += alpha_c * (reward - Q[choice])  # Chosen action update\n",
    "            Q[unchosen] += alpha_u * (reward - Q[choice])  # Unchosen action decay\n",
    "\n",
    "            q_values.append(Q.copy())\n",
    "            q_diff.append(Q[choice] - Q[unchosen])\n",
    "\n",
    "        # print(np.array(q_values).shape)\n",
    "        return np.array(q_values), np.array(q_diff)\n",
    "\n",
    "    # Log-likelihood function to optimize alpha_L and alpha_R\n",
    "    def log_likelihood(params):\n",
    "        alpha_L, alpha_R, beta = params\n",
    "        Q_values, Q_diff = compute_q_values(alpha_L, alpha_R)\n",
    "        # print(Q_diff[:10], Q_diff.shape)\n",
    "\n",
    "        # Predictor for logistic regression: difference in Q-values (Q_right - Q_left)\n",
    "        # X = (Q_values[:, 1] - Q_values[:, 0]).reshape(-1, 1)\n",
    "        # model = LogisticRegression()\n",
    "        # model.fit(X, choices)  # Fit logistic regression on choice data\n",
    "        # probs = model.predict_proba(X)[:, 1]  # Probability of choosing right (1)\n",
    "        Q_diff = (\n",
    "            Q_values[np.arange(len(choices)), choices.astype(int)]\n",
    "            - Q_values[np.arange(len(choices)), (1 - choices).astype(int)]\n",
    "        )\n",
    "        # Q_diff = Q_values[:, 1] - Q_values[:, 0]\n",
    "        probs = 1 / (1 + np.exp(-beta * Q_diff))  # Softmax choice probability\n",
    "        # probs = np.exp(beta * Q_chosen) / (1 + np.exp(-beta * Q_diff))\n",
    "        # Softmax choice probability\n",
    "\n",
    "        eps = 1e-9  # Small constant to prevent log(0) errors\n",
    "        probs = np.clip(probs, eps, 1 - eps)\n",
    "\n",
    "        # print(np.prod(probs))\n",
    "        # Compute log-likelihood\n",
    "        ll = np.sum(choices * np.log(probs) + (1 - choices) * np.log(1 - probs))\n",
    "        # ll = np.sum(np.log(probs))\n",
    "        # ll = np.prod(probs)\n",
    "        return -ll  # Negative for minimization\n",
    "\n",
    "    # Optimize alpha_L and alpha_R using a bounded method\n",
    "    # result = minimize(\n",
    "    #     log_likelihood,\n",
    "    #     x0=[0.63, 0.32, 1.2],\n",
    "    #     bounds=[(0, 1), (-0.3, 1), (1, 10)],\n",
    "    #     method=\"L-BFGS-B\",\n",
    "    #     # method=\"BFGS\",\n",
    "    # )\n",
    "    bads = BADS(\n",
    "        log_likelihood,\n",
    "        x0=np.array([0.3, -0.1, 1.2]),\n",
    "        lower_bounds=np.array([-1, -1, 1]),\n",
    "        upper_bounds=np.array([1, 1, 10]),\n",
    "        plausible_lower_bounds=np.array([0, -0.6, 1]),\n",
    "        plausible_upper_bounds=np.array([0.6, 0.2, 10]),\n",
    "    )\n",
    "    result = bads.optimize()\n",
    "\n",
    "    estimated_params.append(result.x)\n",
    "    alpha_L_est, alpha_R_est, beta = result.x\n",
    "    print(\n",
    "        f\"Estimated alpha_L: {alpha_L_est:.4f}, Estimated alpha_R: {alpha_R_est:.4f}, Estimated: beta: {beta}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuropy import plotting\n",
    "\n",
    "fig = plotting.Fig(1, 2, size=(4, 3), num=1)\n",
    "estimated_params = np.array(estimated_params)\n",
    "ax1 = fig.subplot(fig.gs[0])\n",
    "ax2 = fig.subplot(fig.gs[1])\n",
    "\n",
    "for i in range(estimated_params.shape[0]):\n",
    "    if task_type[i] == \"structured\":\n",
    "        color = \"#5040BF\"\n",
    "    else:\n",
    "        color = \"#AFBF40\"\n",
    "\n",
    "    x1 = np.array([1, 2]) + 0.1 * np.random.randn(2)\n",
    "\n",
    "    ax1.plot(x1, estimated_params[i, :2], \".\", color=color, alpha=0.6)\n",
    "    ax1.set_xlim(0, 3)\n",
    "    ax2.plot(\n",
    "        1 + 0.1 * np.random.randn(1),\n",
    "        estimated_params[i, 2],\n",
    "        \".\",\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    ax2.set_xlim(0, 2)\n",
    "\n",
    "# ax1.legend([\"struc\", \"unstruc\"])\n",
    "ax1.set_xticks([1, 2], [\"Alpha_L\", \"Alpha_R\"])\n",
    "ax2.set_xticks([1], [\"Beta\"])\n",
    "ax1.set_ylabel(\"Estimated alpha values\")\n",
    "ax2.set_ylabel(\"Estimated beta values\")\n",
    "fig.fig.suptitle(\"Q-learning in two-armed bandit task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning optimization of a STOCHASTIC objective function\n",
      "\n",
      " Iteration    f-count      E[f(x)]        SD[f(x)]           MeshScale          Method              Actions\n",
      "     0           1         15.8377             nan               1                                  \n",
      "     0          33       -0.404461             nan               1          Initial mesh            Initial points\n",
      "     0          37       -0.404461               1             0.5          Refine grid             Train\n",
      "     1          45       -0.404461               1            0.25          Refine grid             Train\n",
      "     2          46       -0.292767         0.23357            0.25      Successful search (ES-wcm)        \n",
      "     2          57       -0.292767         0.23357           0.125          Refine grid             Train\n",
      "     3          58        0.130479        0.191946           0.125      Incremental search (ES-wcm)        \n",
      "     3          59        0.107011        0.188473           0.125      Incremental search (ES-wcm)        \n",
      "     3          60       0.0574723        0.185048           0.125      Successful search (ES-wcm)        \n",
      "     3          64        0.019579        0.173409           0.125      Incremental search (ES-wcm)        \n",
      "     3          65     -0.00359517         0.17083           0.125      Incremental search (ES-wcm)        \n",
      "     3          69     -0.00359517         0.17083          0.0625          Refine grid             Train\n",
      "     4          77      -0.0409125        0.158027         0.03125          Refine grid             Train\n",
      "     5          85       0.0270654        0.143492        0.015625          Refine grid             Train\n",
      "     6          86        0.046521        0.128674        0.015625      Successful search (ES-wcm)        \n",
      "     6          91       0.0334791        0.117016        0.015625      Successful search (ES-wcm)        \n",
      "     6          92     -0.00211615        0.116041        0.015625      Successful search (ES-wcm)        \n",
      "     6          93      -0.0167328        0.115092        0.015625      Successful search (ES-wcm)        \n",
      "     6         101      -0.0167328        0.115092       0.0078125          Refine grid             Train\n",
      "     7         105       0.0363469        0.106547       0.0078125      Successful search (ES-wcm)        \n",
      "     7         106       0.0265436        0.105827       0.0078125      Successful search (ES-wcm)        \n",
      "     7         107       0.0159787        0.105121       0.0078125      Successful search (ES-wcm)        \n",
      "     7         108      0.00580935         0.10443       0.0078125      Successful search (ES-wcm)        \n",
      "     7         117      0.00580935         0.10443      0.00390625          Refine grid             \n",
      "     8         122       0.0185388        0.100707       0.0078125        Successful poll           Train\n",
      "     9         123      0.00556943        0.100251       0.0078125      Successful search (ES-wcm)        \n",
      "     9         124      -0.0113642       0.0997586       0.0078125      Successful search (ES-wcm)        \n",
      "     9         129      -0.0251515       0.0971697       0.0078125      Successful search (ES-ell)        \n",
      "     9         131      -0.0336486       0.0962011       0.0078125      Successful search (ES-wcm)        \n",
      "     9         132       -0.048382       0.0958622       0.0078125      Successful search (ES-wcm)        \n",
      "     9         133      -0.0487903       0.0990611       0.0078125      Incremental search (ES-wcm)        \n",
      "     9         135      -0.0507933       0.0977762       0.0078125      Successful search (ES-wcm)        \n",
      "     9         146      -0.0507933       0.0977762      0.00390625          Refine grid             Train\n",
      "    10         147      -0.0540973       0.0883164      0.00390625      Successful search (ES-ell)        \n",
      "    10         158      -0.0540973       0.0883164      0.00195312          Refine grid             Train\n",
      "    11         166      -0.0315363       0.0853129      0.00390625        Successful poll           Train\n",
      "    12         167      -0.0262864       0.0835507      0.00390625      Incremental search (ES-ell)        \n",
      "    12         169      -0.0336098       0.0834646      0.00390625      Successful search (ES-ell)        \n",
      "    12         170      -0.0342946       0.0847229      0.00390625      Incremental search (ES-ell)        \n",
      "    12         174      -0.0390186       0.0831241      0.00390625      Successful search (ES-wcm)        \n",
      "    12         177      -0.0421667       0.0817412      0.00390625      Successful search (ES-wcm)        \n",
      "    12         186      -0.0421667       0.0817412      0.00195312          Refine grid             \n",
      "    13         187    -4.68692e-05        0.078253      0.00195312      Successful search (ES-ell)        \n",
      "    13         188     -0.00344044       0.0776299      0.00195312      Successful search (ES-ell)        \n",
      "    13         189      -0.0107261        0.078361      0.00195312      Successful search (ES-ell)        \n",
      "    13         190       -0.014142       0.0797756      0.00195312      Successful search (ES-ell)        \n",
      "    13         191      -0.0150767       0.0802112      0.00195312      Incremental search (ES-ell)        \n",
      "    13         198       -0.019509       0.0780464      0.00390625        Successful poll           \n",
      "Optimization terminated: change in the function value less than options['tol_fun'].\n",
      "Estimated function value at minimum: 0.1101545782987738 ± 0.10234011463037684 (mean ± SEM from 100 samples)\n",
      "Cannot read version number from package metadata.\n"
     ]
    }
   ],
   "source": [
    "from pybads import BADS\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def noisy_sphere(x, sigma=1.0):\n",
    "    \"\"\"Simple quadratic function with added noise.\"\"\"\n",
    "    x_2d = np.atleast_2d(x)\n",
    "    f = np.sum(x_2d**2, axis=1)\n",
    "    noise = sigma * np.random.normal(size=x_2d.shape[0])\n",
    "    return f + noise\n",
    "\n",
    "\n",
    "x0 = np.array([-3, -3])\n",
    "# Starting point\n",
    "lower_bounds = np.array([-5, -5])\n",
    "upper_bounds = np.array([5, 5])\n",
    "plausible_lower_bounds = np.array([-2, -2])\n",
    "plausible_upper_bounds = np.array([2, 2])\n",
    "\n",
    "options = {\n",
    "    \"uncertainty_handling\": True,\n",
    "    \"max_fun_evals\": 300,\n",
    "    \"noise_final_samples\": 100,\n",
    "}\n",
    "bads = BADS(\n",
    "    noisy_sphere,\n",
    "    x0,\n",
    "    lower_bounds,\n",
    "    upper_bounds,\n",
    "    plausible_lower_bounds,\n",
    "    plausible_upper_bounds,\n",
    "    options=options,\n",
    ")\n",
    "optimize_result = bads.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 2\n",
    "exec(\"D\" + \"=val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    evaluation_parameters = {\"D\": 2}\n",
    "    for key, val in evaluation_parameters.items():\n",
    "        exec(key + \"=val\")\n",
    "    m = D / 2\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"M\": 3}\n",
    "for k, val in a.items():\n",
    "    print(k, val)\n",
    "    exec(k + \"=val\")\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 2\n",
    "exec(\"D=val\")\n",
    "\n",
    "print(D)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
