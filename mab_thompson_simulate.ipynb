{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4e6039",
   "metadata": {},
   "source": [
    "### Beta distribution samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "alpha = 1\n",
    "beta = 1\n",
    "prob = 0.3\n",
    "\n",
    "fig = plotting.Fig(20, 5)\n",
    "for i in range(20):\n",
    "    pdf_values = beta_dist.pdf(x, alpha, beta)\n",
    "\n",
    "    rand_val = np.random.rand()\n",
    "\n",
    "    if rand_val < prob:\n",
    "        alpha += 1\n",
    "    else:\n",
    "        beta += 1\n",
    "\n",
    "    ax = fig.subplot(fig.gs[i])\n",
    "    ax.fill_between(x, pdf_values, alpha=0.5, color=\"green\")\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axvline(prob, color=\"k\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742f787",
   "metadata": {},
   "source": [
    "### Fitting animal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "490bec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.models import Thompson2Arm\n",
    "import mab_subjects\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "exps = mab_subjects.unstruc.allsess + mab_subjects.struc.allsess\n",
    "\n",
    "\n",
    "def get_thomp_param(exp):\n",
    "    grp = \"struc\" if exp.b2a.is_structured else \"unstruc\"\n",
    "\n",
    "    if grp == \"unstruc\":\n",
    "        task = exp.b2a\n",
    "        task.auto_block_window_ids()\n",
    "        reset_bool = task.is_window_start\n",
    "    else:\n",
    "        task = exp.b2a\n",
    "        reset_bool = task.is_session_start\n",
    "\n",
    "    task = task.filter_by_trials(100, 100)\n",
    "    model = Thompson2Arm(task, reset_bool=reset_bool)\n",
    "    model.fit(n_starts=5)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"sub_name\": exp.sub_name,\n",
    "            \"alpha0\": model.alpha0,\n",
    "            \"beta0\": model.beta0,\n",
    "            \"lr_chosen\": model.lr_chosen,\n",
    "            \"lr_unchosen\": model.lr_unchosen,\n",
    "            \"tau\": model.tau,\n",
    "            \"grp\": \"struc\" if exp.b2a.is_structured else \"unstruc\",\n",
    "            \"first_experience\": True if \"Exp1\" in exp.sub_name else False,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    print(\n",
    "        f\"Processed {exp.sub_name} with alpha0={model.alpha0}, beta0={model.beta0}, lr_chosen={model.lr_chosen}, lr_unchosen={model.lr_unchosen}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=6)(delayed(get_thomp_param)(exp) for exp in exps)\n",
    "params_df = pd.concat(results, ignore_index=True)\n",
    "params_df.to_csv(\"thomp_params_reset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49296250",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[-2].b2a.auto_block_window_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[-2].b2a.is_window_start.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = exps[0].b2a.block_ids\n",
    "\n",
    "np.unique(b, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63949824",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[-2].sub_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = exps[0].b2a.datetime.astype(\"datetime64[s]\")\n",
    "\n",
    "# np.diff(a)\n",
    "gap = np.diff(a, prepend=a[0]).astype(\"timedelta64[s]\").astype(int) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.plot(exps[-2].b2a.window_ids)\n",
    "# ax.plot(exps[0].b2a.session_ids)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(exps[-2].b2a.block_ids)\n",
    "\n",
    "# plt.plot(exps[0].b2a.is_block_start / 2)\n",
    "# plt.plot(exps[0].b2a.is_session_start / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[0].b2a.window_ids, exps[0].b2a.session_ids, exps[0].b2a.block_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbea66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4af542b",
   "metadata": {},
   "source": [
    "### V1: Simulating thompson sampling with forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85540696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp(delta_s, delta_f, tau):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    for i in range(100):\n",
    "        # reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "        reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        for tr in range(100):\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            alpha = 1.0 + (alpha - 1.0) * tau\n",
    "            beta = 1.0 + (beta - 1.0) * tau\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                alpha[choice] += delta_s\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                beta[choice] += delta_f\n",
    "                rewards.append(0)\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [[7, 3, 0.5], [5, 5, 0.7], [6, 4, 0.8], [5, 8, 0.9]]\n",
    "for i, (delta_s, delta_f, tau) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids = run_thomp(delta_s, delta_f, tau)\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(f\"deltaS={delta_s}, deltaF={delta_f}, tau={tau}\")\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d333d8",
   "metadata": {},
   "source": [
    "### V2: Simulating thompson sampling with forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0364fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp2(tau, kappa1, kappa2):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    lr = [lr1, lr2]\n",
    "\n",
    "    for i in range(100):\n",
    "        reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "        # reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        s = np.zeros(2)\n",
    "        f = np.zeros(2)\n",
    "\n",
    "        for tr in range(100):\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            s = tau * s\n",
    "            f = tau * f\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                s[choice] += 1 * lr[choice]\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                f[choice] += 1 * lr[choice]\n",
    "                rewards.append(0)\n",
    "\n",
    "            alpha = 1.0 + s\n",
    "            beta = 1.0 + f\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [[0.9, 0.9, 0.9], [0.42, 0.59, 0.47], [0.5, 0.5, 0.5], [0.1, 0.1, 0.5]]\n",
    "for i, (tau, lr1, lr2) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids = run_thomp2(tau, lr1, lr2)\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(f\"tau={tau}, lr1={lr1}, lr2={lr2}\")\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788982c",
   "metadata": {},
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5201ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp2(alpha0, beta0, lr_chosen, lr_unchosen, tau):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    env_type = \"\"\n",
    "\n",
    "    for i in range(100):\n",
    "        rand_env = np.random.rand()\n",
    "        if rand_env <= 0.5:\n",
    "            reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "            env_type = \"structured\"\n",
    "        else:\n",
    "            reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "            env_type = \"unstructured\"\n",
    "\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        s = np.zeros(2)\n",
    "        f = np.zeros(2)\n",
    "\n",
    "        for tr in range(100):\n",
    "            alpha = alpha0 + s\n",
    "            beta = beta0 + f\n",
    "\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            s = tau * s\n",
    "            f = tau * f\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                s[choice] += lr_chosen\n",
    "                f[1 - choice] += lr_unchosen\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                f[choice] += lr_chosen\n",
    "                s[1 - choice] += lr_unchosen\n",
    "                rewards.append(0)\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids, env_type\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [\n",
    "    [1, 1, 0.8, 0.6, 0.8],\n",
    "    [2, 1, 0.5, 0.5, 0.9],\n",
    "    [1, 6, 0.1, 0.8, 0.9],\n",
    "    [5, 5, 0.2, 0.3, 0.7],\n",
    "]\n",
    "for i, (alpha0, beta0, lr_chosen, lr_unchosen, tau) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids, env_type = run_thomp2(\n",
    "        alpha0, beta0, lr_chosen, lr_unchosen, tau\n",
    "    )\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(\n",
    "        f\"env_type={env_type},\\nalpha={alpha0}, beta={beta0},\\nlr_chosen={lr_chosen}, lr_unchosen={lr_unchosen},\\ntau={tau}\"\n",
    "    )\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040713ff",
   "metadata": {},
   "source": [
    "### Smoothness around parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from neuropy import plotting\n",
    "import seaborn as sns\n",
    "from statplotannot.plots import SeabornPlotter\n",
    "from mab_colors import colors_2arm\n",
    "import mab_subjects\n",
    "from banditpy.models import Thompson2Arm\n",
    "\n",
    "file = Path(\"D:/Data/mab/thomp_params_lr_tau.csv\")\n",
    "df = pd.read_csv(file, sep=\",\")\n",
    "# df = df[df[\"first_experience\"] == True]\n",
    "\n",
    "exps = mab_subjects.unstruc.allsess + mab_subjects.struc.allsess\n",
    "\n",
    "for i, exp in enumerate(exps):\n",
    "\n",
    "    task = exp.b2a.filter_by_trials(100, 100)\n",
    "    model = Thompson2Arm(task)\n",
    "    params = df[df[\"sub_name\"] == exp.sub_name]\n",
    "    model.set_params(\n",
    "        lr1=params[\"lr1\"].values[0],\n",
    "        lr2=params[\"lr2\"].values[0],\n",
    "        tau=params[\"tau\"].values[0],\n",
    "    )\n",
    "    model.inspect_smoothness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c57fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.maximum(3, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
