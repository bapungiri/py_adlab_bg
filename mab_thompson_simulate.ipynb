{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4e6039",
   "metadata": {},
   "source": [
    "### Beta distribution samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "alpha = 1\n",
    "beta = 1\n",
    "prob = 0.3\n",
    "\n",
    "fig = plotting.Fig(20, 5)\n",
    "for i in range(20):\n",
    "    pdf_values = beta_dist.pdf(x, alpha, beta)\n",
    "\n",
    "    rand_val = np.random.rand()\n",
    "\n",
    "    if rand_val < prob:\n",
    "        alpha += 1\n",
    "    else:\n",
    "        beta += 1\n",
    "\n",
    "    ax = fig.subplot(fig.gs[i])\n",
    "    ax.fill_between(x, pdf_values, alpha=0.5, color=\"green\")\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axvline(prob, color=\"k\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742f787",
   "metadata": {},
   "source": [
    "### Fitting animal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.models import Thompson2Arm\n",
    "import mab_subjects\n",
    "\n",
    "exps = mab_subjects.unstruc.allsess + mab_subjects.struc.allsess\n",
    "\n",
    "for i, exp in enumerate(exps[:1]):\n",
    "    task = exp.b2a.filter_by_trials(100, 100)\n",
    "    print(exp.sub_name)\n",
    "    model = Thompson2Arm(task)\n",
    "    model.fit(n_starts=2)\n",
    "    model.print_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af542b",
   "metadata": {},
   "source": [
    "### V1: Simulating thompson sampling with forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "85540696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp(delta_s, delta_f, tau):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    for i in range(100):\n",
    "        # reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "        reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        for tr in range(100):\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            alpha = 1.0 + (alpha - 1.0) * tau\n",
    "            beta = 1.0 + (beta - 1.0) * tau\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                alpha[choice] += delta_s\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                beta[choice] += delta_f\n",
    "                rewards.append(0)\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [[7, 3, 0.5], [5, 5, 0.7], [6, 4, 0.8], [5, 8, 0.9]]\n",
    "for i, (delta_s, delta_f, tau) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids = run_thomp(delta_s, delta_f, tau)\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(f\"deltaS={delta_s}, deltaF={delta_f}, tau={tau}\")\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d333d8",
   "metadata": {},
   "source": [
    "### V2: Simulating thompson sampling with forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0364fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp2(tau, kappa1, kappa2):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    lr = [lr1, lr2]\n",
    "\n",
    "    for i in range(100):\n",
    "        reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "        # reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        s = np.zeros(2)\n",
    "        f = np.zeros(2)\n",
    "\n",
    "        for tr in range(100):\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            s = tau * s\n",
    "            f = tau * f\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                s[choice] += 1 * lr[choice]\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                f[choice] += 1 * lr[choice]\n",
    "                rewards.append(0)\n",
    "\n",
    "            alpha = 1.0 + s\n",
    "            beta = 1.0 + f\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [[0.9, 0.9, 0.9], [0.42, 0.59, 0.47], [0.5, 0.5, 0.5], [0.1, 0.1, 0.5]]\n",
    "for i, (tau, lr1, lr2) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids = run_thomp2(tau, lr1, lr2)\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(f\"tau={tau}, lr1={lr1}, lr2={lr2}\")\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788982c",
   "metadata": {},
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5201ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp2(alpha0, beta0, lr_chosen, lr_unchosen, tau):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    env_type = \"\"\n",
    "\n",
    "    for i in range(100):\n",
    "        rand_env = np.random.rand()\n",
    "        if rand_env <= 0.5:\n",
    "            reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "            env_type = \"structured\"\n",
    "        else:\n",
    "            reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "            env_type = \"unstructured\"\n",
    "\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        s = np.zeros(2)\n",
    "        f = np.zeros(2)\n",
    "\n",
    "        for tr in range(100):\n",
    "            alpha = alpha0 + s\n",
    "            beta = beta0 + f\n",
    "\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            s = tau * s\n",
    "            f = tau * f\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                s[choice] += lr_chosen\n",
    "                f[1 - choice] += lr_unchosen\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                f[choice] += lr_chosen\n",
    "                s[1 - choice] += lr_unchosen\n",
    "                rewards.append(0)\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids, env_type\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [\n",
    "    [1, 1, 0.8, 0.6, 0.8],\n",
    "    [2, 1, 0.5, 0.5, 0.9],\n",
    "    [1, 6, 0.1, 0.8, 0.9],\n",
    "    [5, 5, 0.2, 0.3, 0.7],\n",
    "]\n",
    "for i, (alpha0, beta0, lr_chosen, lr_unchosen, tau) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids, env_type = run_thomp2(\n",
    "        alpha0, beta0, lr_chosen, lr_unchosen, tau\n",
    "    )\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(\n",
    "        f\"env_type={env_type},\\nalpha={alpha0}, beta={beta0},\\nlr_chosen={lr_chosen}, lr_unchosen={lr_unchosen},\\ntau={tau}\"\n",
    "    )\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040713ff",
   "metadata": {},
   "source": [
    "### Smoothness around parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NLL=40012.45  SD=45.669  CoefVar=0.0011\n",
      "Mean NLL=21692.14  SD=21.768  CoefVar=0.0010\n",
      "Mean NLL=10870.10  SD=20.782  CoefVar=0.0019\n",
      "Mean NLL=44673.81  SD=29.344  CoefVar=0.0007\n",
      "Mean NLL=9295.66  SD=12.673  CoefVar=0.0014\n",
      "Mean NLL=55071.00  SD=51.968  CoefVar=0.0009\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from neuropy import plotting\n",
    "import seaborn as sns\n",
    "from statplotannot.plots import SeabornPlotter\n",
    "from mab_colors import colors_2arm\n",
    "import mab_subjects\n",
    "from banditpy.models import Thompson2Arm\n",
    "\n",
    "file = Path(\"D:/Data/mab/thomp_params_lr_tau.csv\")\n",
    "df = pd.read_csv(file, sep=\",\")\n",
    "# df = df[df[\"first_experience\"] == True]\n",
    "\n",
    "exps = mab_subjects.unstruc.allsess + mab_subjects.struc.allsess\n",
    "\n",
    "for i, exp in enumerate(exps):\n",
    "\n",
    "    task = exp.b2a.filter_by_trials(100, 100)\n",
    "    model = Thompson2Arm(task)\n",
    "    params = df[df[\"sub_name\"] == exp.sub_name]\n",
    "    model.set_params(\n",
    "        lr1=params[\"lr1\"].values[0],\n",
    "        lr2=params[\"lr2\"].values[0],\n",
    "        tau=params[\"tau\"].values[0],\n",
    "    )\n",
    "    model.inspect_smoothness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c57fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.maximum(3, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
