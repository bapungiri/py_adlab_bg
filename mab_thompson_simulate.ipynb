{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5bf17f",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a2f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGM0\n",
      "Using 5 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting DecisionModel: 100%|██████████| 5/5 [00:00<00:00, 127.87it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lr_c_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 490, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asheshlab\\Documents\\Codes\\BanditPy\\banditpy\\models\\model.py\", line 165, in _run_single\n    res = minimize(\n        self._nll,\n    ...<2 lines>...\n        bounds=bounds,\n    )\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\scipy\\optimize\\_minimize.py\", line 784, in minimize\n    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n                           callback=callback, **options)\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\", line 413, in _minimize_lbfgsb\n    sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n                                  bounds=bounds,\n                                  finite_diff_rel_step=finite_diff_rel_step,\n                                  workers=workers)\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 310, in _prepare_scalar_function\n    sf = ScalarFunction(fun, x0, args, grad, hess,\n                        finite_diff_rel_step, bounds, epsilon=epsilon,\n                        workers=workers)\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 274, in __init__\n    self._update_fun()\n    ~~~~~~~~~~~~~~~~^^\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 353, in _update_fun\n    fx = self._wrapped_fun(self.x)\n  File \"c:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\scipy\\_lib\\_util.py\", line 590, in __call__\n    fx = self.f(np.copy(x), *self.args)\n  File \"C:\\Users\\asheshlab\\Documents\\Codes\\BanditPy\\banditpy\\models\\model.py\", line 114, in _nll\n    self.policy.update(c, r)\n    ~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"C:\\Users\\asheshlab\\Documents\\Codes\\BanditPy\\banditpy\\models\\policy\\thompson.py\", line 137, in update\n    lr_c_pos = p[\"lr_c_pos\"]\n               ~^^^^^^^^^^^^\nKeyError: 'lr_c_pos'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# --- Use this to set custom bounds ---\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# policy.set_bounds(\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#     alpha_c=(-1, 1),\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#     beta=(0.005, 20),\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     26\u001b[39m model = DecisionModel(task, policy=policy)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlbfgs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m model.describe()\n\u001b[32m     29\u001b[39m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Codes\\BanditPy\\banditpy\\models\\model.py:183\u001b[39m, in \u001b[36mDecisionModel.fit\u001b[39m\u001b[34m(self, n_starts, seed, progress, n_jobs, method, de_popsize, de_maxiter, de_tol)\u001b[39m\n\u001b[32m    181\u001b[39m     results = [_run_single(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m iterator]\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloky\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_single\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m best_fun, best_x = \u001b[38;5;28mmin\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m t: t[\u001b[32m0\u001b[39m])\n\u001b[32m    188\u001b[39m fvals = np.array([r[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results], dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py:1784\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1779\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1781\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1785\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1787\u001b[39m     nb_jobs = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py:1859\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1855\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1856\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1859\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py:758\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    752\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    757\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asheshlab\\miniconda3\\envs\\data_analysis\\Lib\\site-packages\\joblib\\parallel.py:773\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'lr_c_pos'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from banditpy.models import DecisionModel\n",
    "from banditpy.models.policy import Thompson2Arm\n",
    "import mab_subjects\n",
    "\n",
    "exps = mab_subjects.mostly_unstruc.allsess + mab_subjects.mostly_unstruc.allsess\n",
    "\n",
    "params_df = []\n",
    "\n",
    "for i, exp in enumerate(exps[:1]):\n",
    "    task = exp.b2a.filter_by_trials(min_trials=100, clip_max=100)\n",
    "    print(exp.sub_name)\n",
    "\n",
    "    policy = Thompson2Arm()\n",
    "    policy.lr_mode = \"split\"\n",
    "    # --- Use this to set custom bounds ---\n",
    "    # policy.set_bounds(\n",
    "    #     alpha_c=(-1, 1),\n",
    "    #     alpha_u=(-1, 1),\n",
    "    #     alpha_h=(0, 1),\n",
    "    #     scaler=(1, 10),\n",
    "    #     beta=(0.005, 20),\n",
    "    # )\n",
    "\n",
    "    model = DecisionModel(task, policy=policy)\n",
    "    model.fit(method=\"lbfgs\", n_jobs=6, n_starts=5, progress=True)\n",
    "    model.describe()\n",
    "    model\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"name\": exp.sub_name,\n",
    "            \"param\": list(model.params.keys()),\n",
    "            \"param_values\": list(model.params.values()),\n",
    "            \"grp\": exp.group_tag,\n",
    "        }\n",
    "    )\n",
    "    params_df.append(df)\n",
    "\n",
    "params_df = pd.concat(params_df, ignore_index=True)\n",
    "# mab_subjects.GroupData().save(params_df, \"qlearn_2alphaH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e6039",
   "metadata": {},
   "source": [
    "### Beta distribution samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta as beta_dist\n",
    "import matplotlib.pyplot as plt\n",
    "from neuropy import plotting\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "alpha = 1\n",
    "beta = 1\n",
    "prob = 0.3\n",
    "\n",
    "fig = plotting.Fig(20, 5)\n",
    "for i in range(20):\n",
    "    pdf_values = beta_dist.pdf(x, alpha, beta)\n",
    "\n",
    "    rand_val = np.random.rand()\n",
    "\n",
    "    if rand_val < prob:\n",
    "        alpha += 1\n",
    "    else:\n",
    "        beta += 1\n",
    "\n",
    "    ax = fig.subplot(fig.gs[i])\n",
    "    ax.fill_between(x, pdf_values, alpha=0.5, color=\"green\")\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axvline(prob, color=\"k\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742f787",
   "metadata": {},
   "source": [
    "### Fitting animal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.models import Thompson2Arm\n",
    "import mab_subjects\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "exps = mab_subjects.unstruc.allsess + mab_subjects.struc.allsess\n",
    "\n",
    "\n",
    "def get_thomp_param(exp):\n",
    "    grp = \"struc\" if exp.b2a.is_structured else \"unstruc\"\n",
    "\n",
    "    if grp == \"unstruc\":\n",
    "        task = exp.b2a\n",
    "        task.auto_block_window_ids()\n",
    "        reset_bool = task.is_window_start\n",
    "    else:\n",
    "        task = exp.b2a\n",
    "        reset_bool = task.is_session_start\n",
    "\n",
    "    task = task.filter_by_trials(100, 100)\n",
    "    model = Thompson2Arm(task, reset_bool=reset_bool)\n",
    "    model.fit(n_starts=5)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"sub_name\": exp.sub_name,\n",
    "            \"alpha0\": model.alpha0,\n",
    "            \"beta0\": model.beta0,\n",
    "            \"lr_chosen\": model.lr_chosen,\n",
    "            \"lr_unchosen\": model.lr_unchosen,\n",
    "            \"tau\": model.tau,\n",
    "            \"grp\": \"struc\" if exp.b2a.is_structured else \"unstruc\",\n",
    "            \"first_experience\": True if \"Exp1\" in exp.sub_name else False,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    print(\n",
    "        f\"Processed {exp.sub_name} with alpha0={model.alpha0}, beta0={model.beta0}, lr_chosen={model.lr_chosen}, lr_unchosen={model.lr_unchosen}\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=6)(delayed(get_thomp_param)(exp) for exp in exps)\n",
    "params_df = pd.concat(results, ignore_index=True)\n",
    "params_df.to_csv(\"thomp_params_reset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49296250",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[-2].b2a.auto_block_window_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[-2].b2a.is_window_start.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = exps[0].b2a.block_ids\n",
    "\n",
    "np.unique(b, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63949824",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[-2].sub_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = exps[0].b2a.datetime.astype(\"datetime64[s]\")\n",
    "\n",
    "# np.diff(a)\n",
    "gap = np.diff(a, prepend=a[0]).astype(\"timedelta64[s]\").astype(int) / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "ax.plot(exps[-2].b2a.window_ids)\n",
    "# ax.plot(exps[0].b2a.session_ids)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(exps[-2].b2a.block_ids)\n",
    "\n",
    "# plt.plot(exps[0].b2a.is_block_start / 2)\n",
    "# plt.plot(exps[0].b2a.is_session_start / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps[0].b2a.window_ids, exps[0].b2a.session_ids, exps[0].b2a.block_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbea66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4af542b",
   "metadata": {},
   "source": [
    "### V1: Simulating thompson sampling with forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85540696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp(delta_s, delta_f, tau):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    for i in range(100):\n",
    "        # reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "        reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        for tr in range(100):\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            alpha = 1.0 + (alpha - 1.0) * tau\n",
    "            beta = 1.0 + (beta - 1.0) * tau\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                alpha[choice] += delta_s\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                beta[choice] += delta_f\n",
    "                rewards.append(0)\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [[7, 3, 0.5], [5, 5, 0.7], [6, 4, 0.8], [5, 8, 0.9]]\n",
    "for i, (delta_s, delta_f, tau) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids = run_thomp(delta_s, delta_f, tau)\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(f\"deltaS={delta_s}, deltaF={delta_f}, tau={tau}\")\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d333d8",
   "metadata": {},
   "source": [
    "### V2: Simulating thompson sampling with forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0364fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp2(tau, kappa1, kappa2):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    lr = [lr1, lr2]\n",
    "\n",
    "    for i in range(100):\n",
    "        reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "        # reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        s = np.zeros(2)\n",
    "        f = np.zeros(2)\n",
    "\n",
    "        for tr in range(100):\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            s = tau * s\n",
    "            f = tau * f\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                s[choice] += 1 * lr[choice]\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                f[choice] += 1 * lr[choice]\n",
    "                rewards.append(0)\n",
    "\n",
    "            alpha = 1.0 + s\n",
    "            beta = 1.0 + f\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [[0.9, 0.9, 0.9], [0.42, 0.59, 0.47], [0.5, 0.5, 0.5], [0.1, 0.1, 0.5]]\n",
    "for i, (tau, lr1, lr2) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids = run_thomp2(tau, lr1, lr2)\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(f\"tau={tau}, lr1={lr1}, lr2={lr2}\")\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788982c",
   "metadata": {},
   "source": [
    "### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5201ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from banditpy.core import Bandit2Arm\n",
    "from banditpy.plots import plot_trial_by_trial_2Arm\n",
    "from neuropy import plotting\n",
    "\n",
    "n_sim = 500\n",
    "# probs = np.arange(0.1, 1, 0.1)\n",
    "probs = [0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def run_thomp2(alpha0, beta0, lr_chosen, lr_unchosen, tau):\n",
    "\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    reward_probs = []\n",
    "    session_ids = []\n",
    "    env_type = \"\"\n",
    "\n",
    "    for i in range(100):\n",
    "        rand_env = np.random.rand()\n",
    "        if rand_env <= 0.5:\n",
    "            reward_probs_i = [p := np.random.choice(probs), 1 - p]\n",
    "            env_type = \"structured\"\n",
    "        else:\n",
    "            reward_probs_i = np.random.choice(probs, size=2, replace=False)\n",
    "            env_type = \"unstructured\"\n",
    "\n",
    "        alpha = np.ones(2)\n",
    "        beta = np.ones(2)\n",
    "        s = np.zeros(2)\n",
    "        f = np.zeros(2)\n",
    "\n",
    "        for tr in range(100):\n",
    "            alpha = alpha0 + s\n",
    "            beta = beta0 + f\n",
    "\n",
    "            samples = np.random.beta(alpha[:, None], beta[:, None], size=(2, n_sim))\n",
    "            selected = np.argmax(samples, axis=0)\n",
    "            choice_prob = np.array([1 - selected.mean(), selected.mean()])\n",
    "            choice = np.random.choice([0, 1], p=choice_prob)\n",
    "            random_num = np.random.rand()\n",
    "\n",
    "            s = tau * s\n",
    "            f = tau * f\n",
    "\n",
    "            if random_num < reward_probs_i[choice]:\n",
    "                s[choice] += lr_chosen\n",
    "                f[1 - choice] += lr_unchosen\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                f[choice] += lr_chosen\n",
    "                s[1 - choice] += lr_unchosen\n",
    "                rewards.append(0)\n",
    "\n",
    "            choices.append(choice)\n",
    "            session_ids.append(i)\n",
    "            reward_probs.append(reward_probs_i)\n",
    "\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    reward_probs = np.array(reward_probs)\n",
    "    session_ids = np.array(session_ids)\n",
    "\n",
    "    return choices, rewards, reward_probs, session_ids, env_type\n",
    "\n",
    "\n",
    "fig = plotting.Fig(8, 4, fontsize=10)\n",
    "\n",
    "params = [\n",
    "    [1, 1, 0.8, 0.6, 0.8],\n",
    "    [2, 1, 0.5, 0.5, 0.9],\n",
    "    [1, 6, 0.1, 0.8, 0.9],\n",
    "    [5, 5, 0.2, 0.3, 0.7],\n",
    "]\n",
    "for i, (alpha0, beta0, lr_chosen, lr_unchosen, tau) in enumerate(params):\n",
    "    choices, rewards, reward_probs, session_ids, env_type = run_thomp2(\n",
    "        alpha0, beta0, lr_chosen, lr_unchosen, tau\n",
    "    )\n",
    "    task = Bandit2Arm(\n",
    "        probs=reward_probs, choices=choices, rewards=rewards, session_ids=session_ids\n",
    "    )\n",
    "    perf = task.get_optimal_choice_probability()\n",
    "    ax = fig.subplot(fig.gs[:3, i])\n",
    "    plot_trial_by_trial_2Arm(task, ax=ax, sort_by_deltaprob=True)\n",
    "    ax.set_title(\n",
    "        f\"env_type={env_type},\\nalpha={alpha0}, beta={beta0},\\nlr_chosen={lr_chosen}, lr_unchosen={lr_unchosen},\\ntau={tau}\"\n",
    "    )\n",
    "\n",
    "    ax2 = fig.subplot(fig.gs[3, i])\n",
    "    ax2.plot(np.arange(100), perf, color=\"k\")\n",
    "    ax2.set_ylim(0.4, 1.0)\n",
    "    ax2.set_xlabel(\"Trial\")\n",
    "    ax2.set_ylabel(\"Pr(High)\")\n",
    "    ax2.grid(axis=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040713ff",
   "metadata": {},
   "source": [
    "### Smoothness around parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from neuropy import plotting\n",
    "import seaborn as sns\n",
    "from statplotannot.plots import SeabornPlotter\n",
    "from mab_colors import colors_2arm\n",
    "import mab_subjects\n",
    "from banditpy.models import Thompson2Arm\n",
    "\n",
    "file = Path(\"D:/Data/mab/thomp_params_lr_tau.csv\")\n",
    "df = pd.read_csv(file, sep=\",\")\n",
    "# df = df[df[\"first_experience\"] == True]\n",
    "\n",
    "exps = mab_subjects.unstruc.allsess + mab_subjects.struc.allsess\n",
    "\n",
    "for i, exp in enumerate(exps):\n",
    "\n",
    "    task = exp.b2a.filter_by_trials(100, 100)\n",
    "    model = Thompson2Arm(task)\n",
    "    params = df[df[\"sub_name\"] == exp.sub_name]\n",
    "    model.set_params(\n",
    "        lr1=params[\"lr1\"].values[0],\n",
    "        lr2=params[\"lr2\"].values[0],\n",
    "        tau=params[\"tau\"].values[0],\n",
    "    )\n",
    "    model.inspect_smoothness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c57fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.maximum(3, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
